{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회\n",
    "\n",
    "**※주의** : 반드시 본 파일을 이용하여 제출을 수행해야 하며, 파일의 이름은 `task.ipynb`로 유지되어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### 추론 실행 환경\n",
    "    * `python 3.10` 환경\n",
    "    * `CUDA 12.6`를 지원합니다.\n",
    "    * torch 버전: `2.7.1`\n",
    "\n",
    "| Python | CUDA | torch |\n",
    "|--------|------|-------|\n",
    "| 3.10   | 12.6 | 2.7.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### `task.ipynb` 작성 규칙\n",
    "코드는 크게 3가지 파트로 구성되며, 해당 파트의 특성을 지켜서 내용을 편집하세요.   \n",
    "1. **제출용 aifactory 라이브러리 및 추가 필요 라이브러리 설치**\n",
    "    - 채점 및 제출을 위한 aifactory 라이브러리를 설치하는 셀입니다. 이 부분은 수정하지 않고 그대로 실행합니다.\n",
    "    - 그 외로, 모델 추론에 필요한 라이브러리를 직접 설치합니다.\n",
    "2. **추론용 코드 작성**\n",
    "    - 모델 로드, 데이터 전처리, 예측 등 실제 추론을 수행하는 모든 코드를 이 영역에 작성합니다.\n",
    "3. **aif.submit() 함수를 호출하여 최종 결과를 제출**\n",
    "    - **마이 페이지-활동히스토리**에서 발급받은 key 값을 함수의 인자로 정확히 입력해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 제출용 aifactory 라이브러리 설치\n",
    "※ 결과 전송에 필요하므로 아래와 같이 aifactory 라이브러리가 반드시 최신버전으로 설치될 수 있게끔 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: aifactory in /home/seonmin/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: pipreqs in /home/seonmin/.local/lib/python3.10/site-packages (from aifactory) (0.5.0)\n",
      "Requirement already satisfied: ipynbname in /home/seonmin/.local/lib/python3.10/site-packages (from aifactory) (2025.8.0.0)\n",
      "Requirement already satisfied: gdown in /home/seonmin/.local/lib/python3.10/site-packages (from aifactory) (5.2.0)\n",
      "Requirement already satisfied: requests in /home/seonmin/.local/lib/python3.10/site-packages (from aifactory) (2.32.5)\n",
      "Requirement already satisfied: IPython in /home/seonmin/.local/lib/python3.10/site-packages (from aifactory) (8.12.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/seonmin/.local/lib/python3.10/site-packages (from gdown->aifactory) (4.14.2)\n",
      "Requirement already satisfied: filelock in /home/seonmin/.local/lib/python3.10/site-packages (from gdown->aifactory) (3.20.0)\n",
      "Requirement already satisfied: tqdm in /home/seonmin/.local/lib/python3.10/site-packages (from gdown->aifactory) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/seonmin/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown->aifactory) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/seonmin/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown->aifactory) (4.15.0)\n",
      "Requirement already satisfied: ipykernel in /home/seonmin/.local/lib/python3.10/site-packages (from ipynbname->aifactory) (7.1.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (7.1.2)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/seonmin/.local/lib/python3.10/site-packages (from ipykernel->ipynbname->aifactory) (5.14.3)\n",
      "Requirement already satisfied: backcall in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (0.19.2)\n",
      "Requirement already satisfied: pickleshare in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (2.19.2)\n",
      "Requirement already satisfied: stack-data in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/seonmin/.local/lib/python3.10/site-packages (from IPython->aifactory) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /home/seonmin/.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->aifactory) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/seonmin/.local/lib/python3.10/site-packages (from jedi>=0.16->IPython->aifactory) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/seonmin/.local/lib/python3.10/site-packages (from jupyter-client>=8.0.0->ipykernel->ipynbname->aifactory) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/seonmin/.local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->ipynbname->aifactory) (4.5.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/seonmin/.local/lib/python3.10/site-packages (from pexpect>4.3->IPython->aifactory) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->ipynbname->aifactory) (1.16.0)\n",
      "Requirement already satisfied: docopt==0.6.2 in /home/seonmin/.local/lib/python3.10/site-packages (from pipreqs->aifactory) (0.6.2)\n",
      "Requirement already satisfied: nbconvert<8.0.0,>=7.11.0 in /home/seonmin/.local/lib/python3.10/site-packages (from pipreqs->aifactory) (7.16.6)\n",
      "Requirement already satisfied: yarg==0.1.9 in /home/seonmin/.local/lib/python3.10/site-packages (from pipreqs->aifactory) (0.1.9)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/seonmin/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.1.6)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/lib/python3/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2.0.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/seonmin/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /home/seonmin/.local/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/seonmin/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/seonmin/.local/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/seonmin/.local/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (4.25.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/seonmin/.local/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/seonmin/.local/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/seonmin/.local/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/seonmin/.local/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.28.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/seonmin/.local/lib/python3.10/site-packages (from requests->aifactory) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->aifactory) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/seonmin/.local/lib/python3.10/site-packages (from requests->aifactory) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->aifactory) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/seonmin/.local/lib/python3.10/site-packages (from requests[socks]->gdown->aifactory) (1.7.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/seonmin/.local/lib/python3.10/site-packages (from stack-data->IPython->aifactory) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/seonmin/.local/lib/python3.10/site-packages (from stack-data->IPython->aifactory) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/seonmin/.local/lib/python3.10/site-packages (from stack-data->IPython->aifactory) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U aifactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 자신의 모델 추론 실행에 필요한 추가 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSFM 모델 추론에 필요한 라이브러리 설치\n",
    "# 대회 서버 환경: Python 3.10 + CUDA 12.6 + torch 2.7.1 (기본 설치)\n",
    "\n",
    "!pip install timm==0.4.5 --no-cache-dir --quiet\n",
    "!pip install opencv-python-headless==4.10.0.82 --no-cache-dir --quiet\n",
    "!pip install numpy==1.26.4 --no-cache-dir --quiet\n",
    "!pip install Pillow==10.0.0 --no-cache-dir --quiet\n",
    "!pip install mediapipe==0.10.9 --no-cache-dir --quiet\n",
    "!pip install tqdm==4.66.1 --no-cache-dir --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 추론용 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 추론 환경의 기본 경로 구조\n",
    "\n",
    "- 평가 데이터셋 경로: `./data/`\n",
    "   - 채점에 사용될 테스트 데이터셋은 `./data/` 디렉토리 안에 포함되어 있습니다.\n",
    "   - 해당 디렉토리에는 이미지(JPG, PNG)와 동영상(MP4) 파일이 별도의 하위 폴더 없이 혼합되어 있습니다.\n",
    "```bash\n",
    "/aif/\n",
    "└── data/\n",
    "    ├── {이미지 데이터1}.jpg\n",
    "    ├── {이미지 데이터2}.png\n",
    "    ├── {동영상 데이터1}.mp4\n",
    "    ├── {이미지 데이터3}.png\n",
    "    ├── {동영상 데이터2}.mp4\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 및 자원 경로: 예시 : `./model/`\n",
    "   - 추론 스크립트가 실행되는 위치를 기준으로, 제출된 모델 관련 파일들이 위치해야 하는 상대 경로입니다.\n",
    "   - 학습된 모델 가중치(.pt, .ckpt, .pth 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 제출 파일은 `submission.csv`로 저장돼야 합니다.\n",
    "  * submission.csv는 *filename*과 *label* 컬럼으로 구성돼야 합니다.\n",
    "  * filename은 추론한 파일의 이름(확장자 포함), label은 추론 결과입니다. (real:0, fake:1)\n",
    "  * filename은 *string*, label은 *int* 자료형이어야 합니다.\n",
    "\n",
    "| filename | label |\n",
    "|----------|-------|\n",
    "| {이미지 데이터1}.jpg | 0 |\n",
    "| {동영상 데이터1}.mp4 | 1 |\n",
    "| ... | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**※ 주의 사항**\n",
    "\n",
    "* argparse 사용시 `args, _ = parser.parse_known_args()`로 인자를 지정하세요.   \n",
    "   - `args = parser.parse_args()`는 jupyter에서 오류가 발생합니다.\n",
    "* return 할 결과물과 양식에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "import argparse\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mediapipe 로그 억제\n",
    "os.environ['GLOG_minloglevel'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    MEDIAPIPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MEDIAPIPE_AVAILABLE = False\n",
    "\n",
    "# ============================================================\n",
    "# FSFM Vision Transformer Model Definition\n",
    "# ============================================================\n",
    "import timm.models.vision_transformer\n",
    "\n",
    "class VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n",
    "    \"\"\" Vision Transformer with support for global average pooling \"\"\"\n",
    "    def __init__(self, global_pool=False, **kwargs):\n",
    "        super(VisionTransformer, self).__init__(**kwargs)\n",
    "        self.global_pool = global_pool\n",
    "        if self.global_pool:\n",
    "            norm_layer = kwargs['norm_layer']\n",
    "            embed_dim = kwargs['embed_dim']\n",
    "            self.fc_norm = norm_layer(embed_dim)\n",
    "            del self.norm\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.global_pool:\n",
    "            x = x[:, 1:, :].mean(dim=1)\n",
    "            outcome = self.fc_norm(x)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "            outcome = x[:, 0]\n",
    "        return outcome\n",
    "\n",
    "def vit_base_patch16(**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# Face Detection Utilities (Mediapipe + Haar Cascade only)\n",
    "# ============================================================\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"}\n",
    "VIDEO_EXTS = {\".avi\", \".mp4\", \".AVI\", \".MP4\"}\n",
    "\n",
    "HAAR_FACE_CASCADE = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "if MEDIAPIPE_AVAILABLE:\n",
    "    try:\n",
    "        mp_face_detection = mp.solutions.face_detection\n",
    "        MEDIAPIPE_DETECTOR = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.3)\n",
    "    except:\n",
    "        MEDIAPIPE_DETECTOR = None\n",
    "        MEDIAPIPE_AVAILABLE = False\n",
    "else:\n",
    "    MEDIAPIPE_DETECTOR = None\n",
    "\n",
    "def detect_face_haar(image_np, target_size=(224, 224)):\n",
    "    \"\"\"Face detection using OpenCV Haar Cascade\"\"\"\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    faces = HAAR_FACE_CASCADE.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "    expansion = int(max(w, h) * 0.3)\n",
    "    x = max(0, x - expansion)\n",
    "    y = max(0, y - expansion)\n",
    "    w = min(image_np.shape[1] - x, w + 2 * expansion)\n",
    "    h = min(image_np.shape[0] - y, h + 2 * expansion)\n",
    "    cropped_np = image_np[y:y + h, x:x + w]\n",
    "    face_img = Image.fromarray(cropped_np).resize(target_size, Image.BICUBIC)\n",
    "    return face_img\n",
    "\n",
    "def detect_face_mediapipe(image_np, target_size=(224, 224)):\n",
    "    \"\"\"Face detection using Mediapipe\"\"\"\n",
    "    if MEDIAPIPE_DETECTOR is None:\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2RGB)\n",
    "    results = MEDIAPIPE_DETECTOR.process(image_rgb)\n",
    "    if not results.detections:\n",
    "        return None\n",
    "    detection = max(results.detections, key=lambda d: d.score[0])\n",
    "    h, w = image_np.shape[:2]\n",
    "    bbox = detection.location_data.relative_bounding_box\n",
    "    x1 = int(bbox.xmin * w)\n",
    "    y1 = int(bbox.ymin * h)\n",
    "    x2 = int((bbox.xmin + bbox.width) * w)\n",
    "    y2 = int((bbox.ymin + bbox.height) * h)\n",
    "    margin = int(max(x2 - x1, y2 - y1) * 0.1)\n",
    "    x1 = max(0, x1 - margin)\n",
    "    y1 = max(0, y1 - margin)\n",
    "    x2 = min(w, x2 + margin)\n",
    "    y2 = min(h, y2 + margin)\n",
    "    cropped_np = image_np[y1:y2, x1:x2]\n",
    "    if cropped_np.size == 0:\n",
    "        return None\n",
    "    face_img = Image.fromarray(cropped_np).resize(target_size, Image.BICUBIC)\n",
    "    return face_img\n",
    "\n",
    "def detect_and_crop_face_multi(image: Image.Image, target_size=(224, 224)):\n",
    "    \"\"\"Multi-method face detection with fallback (Mediapipe -> Haar -> Full Image)\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Try Mediapipe first\n",
    "    if MEDIAPIPE_AVAILABLE:\n",
    "        try:\n",
    "            face_img = detect_face_mediapipe(image_np, target_size)\n",
    "            if face_img:\n",
    "                return face_img\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try Haar Cascade\n",
    "    try:\n",
    "        face_img = detect_face_haar(image_np, target_size)\n",
    "        if face_img:\n",
    "            return face_img\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: resize original image\n",
    "    resized_img = Image.fromarray(image_np).resize(target_size, Image.BICUBIC)\n",
    "    return resized_img\n",
    "\n",
    "def process_video_frames(video_path, num_frames=10, max_duration=10):\n",
    "    \"\"\"Extract and process frames from video\"\"\"\n",
    "    face_images = []\n",
    "    cap = None\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            return face_images\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            return face_images\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps > 0:\n",
    "            max_frames = int(fps * max_duration)\n",
    "            total_frames = min(total_frames, max_frames)\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            face_img = detect_and_crop_face_multi(image)\n",
    "            if face_img:\n",
    "                face_images.append(face_img)\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        if cap is not None:\n",
    "            cap.release()\n",
    "    return face_images\n",
    "\n",
    "# ============================================================\n",
    "# Main Inference Logic\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\" or True:\n",
    "    print(\"Starting inference...\")\n",
    "\n",
    "\n",
    "    # Model path and test data path\n",
    "    model_weights_path = \"./model/fsfm_vit_base_checkpoint.pth\"\n",
    "    test_dataset_path = Path(\"./data\")\n",
    "    output_csv_path = Path(\"submission.csv\")\n",
    "\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = vit_base_patch16(num_classes=2, global_pool=True, drop_path_rate=0.1)\n",
    "    \n",
    "    # Load checkpoint with safe globals for PyTorch 2.6+\n",
    "    torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "    checkpoint = torch.load(model_weights_path, map_location='cpu', weights_only=True)\n",
    "    if 'model' in checkpoint:\n",
    "        checkpoint_model = checkpoint['model']\n",
    "    else:\n",
    "        checkpoint_model = checkpoint\n",
    "    \n",
    "    # Remove head weights if shape mismatch\n",
    "    state_dict = model.state_dict()\n",
    "    for k in ['head.weight', 'head.bias']:\n",
    "        if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "            print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "            del checkpoint_model[k]\n",
    "    \n",
    "    load_result = model.load_state_dict(checkpoint_model, strict=False)\n",
    "\n",
    "\n",
    "    # Device setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model ready on {device}\")\n",
    "\n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Get all files from data directory (including subdirectories)\n",
    "    files = []\n",
    "    dir_stats = {}\n",
    "    for root, dirs, filenames in os.walk(test_dataset_path):\n",
    "        root_path = Path(root)\n",
    "        rel_path = root_path.relative_to(test_dataset_path) if root_path != test_dataset_path else Path(\".\")\n",
    "        dir_stats[str(rel_path)] = len(filenames)\n",
    "        for filename in filenames:\n",
    "            files.append(Path(root) / filename)\n",
    "    \n",
    "    total_files = len(files)\n",
    "    print(f\"Processing {total_files} files\")\n",
    "    for dir_name, count in sorted(dir_stats.items()):\n",
    "        print(f\"  {dir_name}: {count} files\")\n",
    "    \n",
    "    # CSV header\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"label\"])\n",
    "\n",
    "    num_frames_to_extract = 10\n",
    "    results = []\n",
    "    error_count = 0\n",
    "    skipped_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process files\n",
    "    for idx, file_path in enumerate(tqdm(files, desc=\"Processing\", ncols=80)):\n",
    "        face_images = []\n",
    "        ext = file_path.suffix.lower()\n",
    "        predicted_class = 0\n",
    "\n",
    "        try:\n",
    "            if ext in IMAGE_EXTS:\n",
    "                image = Image.open(file_path)\n",
    "                face_img = detect_and_crop_face_multi(image)\n",
    "                if face_img:\n",
    "                    face_images = [face_img]\n",
    "\n",
    "            elif ext in VIDEO_EXTS:\n",
    "                face_images = process_video_frames(file_path, num_frames_to_extract, max_duration=10)\n",
    "                if len(face_images) == 0:\n",
    "                    # Fallback: use first frame\n",
    "                    try:\n",
    "                        cap = cv2.VideoCapture(str(file_path))\n",
    "                        ret, frame = cap.read()\n",
    "                        cap.release()\n",
    "                        if ret:\n",
    "                            fallback_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                            face_img = detect_and_crop_face_multi(fallback_image)\n",
    "                            if face_img:\n",
    "                                face_images = [face_img]\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                # Unknown extension - still process as image attempt\n",
    "                try:\n",
    "                    image = Image.open(file_path)\n",
    "                    face_img = detect_and_crop_face_multi(image)\n",
    "                    if face_img:\n",
    "                        face_images = [face_img]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Inference\n",
    "            if len(face_images) > 0:\n",
    "                with torch.no_grad():\n",
    "                    # Use all available frames (up to 10 for videos)\n",
    "                    batch = face_images[:min(len(face_images), 10)]\n",
    "                    img_tensors = torch.stack([transform(img) for img in batch]).to(device)\n",
    "                    \n",
    "                    # Forward pass for each frame\n",
    "                    logits_list = []\n",
    "                    for img_tensor in img_tensors:\n",
    "                        logits = model(img_tensor.unsqueeze(0))\n",
    "                        logits_list.append(logits)\n",
    "                    \n",
    "                    # Average predictions across frames\n",
    "                    avg_logits = torch.mean(torch.cat(logits_list, dim=0), dim=0, keepdim=True)\n",
    "                    probs = F.softmax(avg_logits, dim=1)\n",
    "                    predicted_class = torch.argmax(probs).item()\n",
    "                    \n",
    "                    # GPU memory cleanup\n",
    "                    del img_tensors, logits_list, avg_logits, probs\n",
    "                    if device == \"cuda\":\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            predicted_class = 0\n",
    "\n",
    "        # Store result - ALWAYS store a result for every file\n",
    "        results.append([file_path.name, int(predicted_class)])\n",
    "\n",
    "\n",
    "\n",
    "    # Final statistics\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"\\nCompleted {len(results)} files in {elapsed_total/60:.1f} min\")\n",
    "\n",
    "    # Write results to CSV\n",
    "    with open(output_csv_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # CSV validation\n",
    "    with open(output_csv_path, mode=\"r\") as f:\n",
    "        data_rows = sum(1 for _ in f) - 1\n",
    "    print(f\"CSV: {data_rows}/{total_files} rows | {'✓ OK' if data_rows == total_files else '✗ MISMATCH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `aif.submit()` 함수를 호출하여 최종 결과를 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**※주의** : task별, 참가자별로 key가 다릅니다. 잘못 입력하지 않도록 유의바랍니다.\n",
    "- key는 대회 페이지 [베이스라인 코드](https://aifactory.space/task/9197/baseline) 탭에 기재된 가이드라인을 따라 task 별로 확인하실 수 있습니다.\n",
    "- key가 틀리면 제출이 진행되지 않거나 잘못 제출되므로 task에 맞는 자신의 key를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : task\n",
      "jupyter notebook\n",
      "제출 완료\n",
      "283.56489753723145\n"
     ]
    }
   ],
   "source": [
    "import aifactory.score as aif\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "#-----------------------------------------------------#\n",
    "aif.submit(model_name=\"FSFM-ViT-Base\",\n",
    "    key=\"cae0fbcb-0410-4084-a308-21c98d8d886b\"  # ← 여기에 본인의 key를 입력하세요\n",
    ")\n",
    "#-----------------------------------------------------#\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
